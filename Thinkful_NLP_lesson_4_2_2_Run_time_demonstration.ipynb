{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thinkful NLP lesson 4.2.2 Run time demonstration",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zdwhite/Thinkful-Unit-2/blob/master/Thinkful_NLP_lesson_4_2_2_Run_time_demonstration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cCNiTspxB9vv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Instructions\n",
        "\n",
        "\n",
        "1.  Run the first code cell.\n",
        "2.  Then click runtime at the top and completely restart the runtime and skip to the second code block and continue execurint codeblocks as instructed\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Jbc7R1JcSkYI",
        "colab_type": "code",
        "outputId": "33d2a0bf-5afa-4596-d81d-b485a3d1724f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Collecting numpy>=1.15.0 (from spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/9d804d2348471c67a7d8b5f84f9bc59fd1cefa148986f2b74552f8573555/numpy-1.15.4-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 13.9MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.28.1)\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
            "Installing collected packages: numpy\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "Successfully installed numpy-1.15.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uH5kvI7pJn4Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**REMINDER TO RESTART RUNTIME BEFORE RUNNING THE NEXT CODE BLOCK AND NOT AFTER (Restart and NOT Reset)**"
      ]
    },
    {
      "metadata": {
        "id": "z58H3e4QUFZ2",
        "colab_type": "code",
        "outputId": "e258399e-ea67-4fb3-b488-190f15c8a15f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.4MB 36.2MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4wJ-7gafSqh5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "fc0c60fe-1274-426d-dd04-b5d3ea019251"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Launch the installer to download \"gutenberg\" and \"stop words\" corpora.\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "Re5x9y2zRwZG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dLZTHbr-SKD0",
        "colab_type": "code",
        "outputId": "a10f1773-d90f-4e16-d009-1b7c4da65825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "# Import the data we just downloaded and installed.\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "\n",
        "# Grab and process the raw data.\n",
        "print(gutenberg.fileids())\n",
        "\n",
        "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
        "alice = gutenberg.raw('carroll-alice.txt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ti7db7WNRxwr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function for standard text cleaning.\n",
        "def text_cleaner(text):\n",
        "    # Visual inspection identifies a form of punctuation spaCy does not\n",
        "    # recognize: the double dash '--'.  Better get rid of it now!\n",
        "    text = re.sub(r'--',' ',text)\n",
        "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "    \n",
        "# Load and clean the data.\n",
        "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
        "alice = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# The Chapter indicator is idiosyncratic\n",
        "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
        "alice = re.sub(r'CHAPTER .*', '', alice)\n",
        "    \n",
        "alice = text_cleaner(alice)\n",
        "persuasion = text_cleaner(persuasion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pUQDmbi6CtBx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Timing the code on a host VM runtime\n",
        "\n",
        "Below is a code block that imports a module to time and get a sense of how long it's going to take to run certain functions\n",
        "\n",
        "**EVERYTHING ABOVE THIS SECTION SHOULD BE EXECUTED AFTER THE RUNTIME HAS BEEN RESTARTED**"
      ]
    },
    {
      "metadata": {
        "id": "O4OIlOdNC3FK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2QD1bh4BTx7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dcca6899-f3c0-4b70-ab40-a57484251bda"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "start = timeit.default_timer()\n",
        "\n",
        "# All the processing work is done here, so it may take a while.\n",
        "alice_doc = nlp(alice)\n",
        "persuasion_doc = nlp(persuasion)\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "\n",
        "print('Time: ', stop - start) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  33.151894218999985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7AQVkJKFKA0Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This part isn't TOO bad, for me it's ~30 -40 seconds of runtime. Nothing to really worry about."
      ]
    },
    {
      "metadata": {
        "id": "OcyYlYdoCr12",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aBREp8JfUllf",
        "colab_type": "code",
        "outputId": "a3589d68-0c06-4bd5-d040-adb10bb33231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "cell_type": "code",
      "source": [
        "# Group into sentences.\n",
        "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
        "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
        "\n",
        "# Combine the sentences from the two novels into one data frame.\n",
        "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
        "sentences.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(I, shall, be, late, !, ')</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0        1\n",
              "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
              "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
              "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
              "3                                      (Oh, dear, !)  Carroll\n",
              "4                         (I, shall, be, late, !, ')  Carroll"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "JGlhllqLUrhT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " #Utility function to create a list of the 2000 most common words.\n",
        "def bag_of_words(text):\n",
        "    \n",
        "    # Filter out punctuation and stop words.\n",
        "    allwords = [token.lemma_\n",
        "                for token in text\n",
        "                if not token.is_punct\n",
        "                and not token.is_stop]\n",
        "    \n",
        "    # Return the most common words.\n",
        "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
        "    \n",
        "\n",
        "# Creates a data frame with features for each word in our common word set.\n",
        "# Each value is the count of the times the word appears in each sentence.\n",
        "def bow_features(sentences, common_words):\n",
        "    start = timeit.default_timer()\n",
        "    # Scaffold the data frame and initialize counts to zero.\n",
        "    df = pd.DataFrame(columns=common_words)\n",
        "    df['text_sentence'] = sentences[0]\n",
        "    df['text_source'] = sentences[1]\n",
        "    df.loc[:, common_words] = 0\n",
        "    \n",
        "    # Process each row, counting the occurrence of words in each sentence.\n",
        "    for i, sentence in enumerate(df['text_sentence']):\n",
        "        \n",
        "        # Convert the sentence to lemmas, then filter out punctuation,\n",
        "        # stop words, and uncommon words.\n",
        "        words = [token.lemma_\n",
        "                 for token in sentence\n",
        "                 if (\n",
        "                     not token.is_punct\n",
        "                     and not token.is_stop\n",
        "                     and token.lemma_ in common_words\n",
        "                 )]\n",
        "        \n",
        "        # Populate the row with word counts.\n",
        "        for word in words:\n",
        "            df.loc[i, word] += 1\n",
        "        \n",
        "        # This counter is just to make sure the kernel didn't hang.\n",
        "        if i % 50 == 0:\n",
        "            stop = timeit.default_timer()\n",
        "            time = (stop - start)/60\n",
        "            print(\"Processing row {}\".format(i))\n",
        "            print(\"\\n Current runtime to process row {} is {} in minutes\".format(i,time))\n",
        "    stop = timeit.default_timer()\n",
        "    total = (stop - start)/60\n",
        "    return df, print(total)\n",
        "\n",
        "# Set up the bags.\n",
        "alicewords = bag_of_words(alice_doc)\n",
        "persuasionwords = bag_of_words(persuasion_doc)\n",
        "\n",
        "# Combine bags to create a set of unique words.\n",
        "common_words = set(alicewords + persuasionwords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "izoGW82-DR2Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The real hog.\n",
        "\n",
        "Above is an example of a function that is going to take a LONG time to run. We know this just by looking at the structure\n",
        "\n",
        "\n",
        "\n",
        ">1.    Some variables are initialized, nothing really to see here.\n",
        "2.   Then we loop over every sentence with in the book of Alice in wonderland.\n",
        "3.    With in each sentence 2 opperations are checked, then yet another loop is nested where by the word(token in this case) is checked against a list of common words.\n",
        "4.    This means that there is 3 nested iterables which means that as the size of X (X being the length of data that increases the run time increases by the power of 3 or X^3)\n",
        "5.    Lastly a second nested loop checks each word and the number of times it appears with in the list of words (this loop is of neglable influence on runtime as X becomes larger)\n",
        "6.    What we should observe when invoking this function, is that for relatively small lengths of X their run times for every 50 rows should be exponentially shorter than large lenths of X. **I've added a way to test this below**\n"
      ]
    },
    {
      "metadata": {
        "id": "HihF-oSLR3wx",
        "colab_type": "code",
        "outputId": "1cb5d66c-19be-4bb2-f5ca-3bbc80ed3e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "cell_type": "code",
      "source": [
        "#First let's run 200 rows of data which will produce 3 splits and a total run time\n",
        "x = 200\n",
        "test = sentences.iloc[:x,:]\n",
        "#test = sentences\n",
        "#start = timeit.default_timer()\n",
        "\n",
        "# Create our data frame with features. This can take a while to run.\n",
        "word_counts = bow_features(test, common_words)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing row 0\n",
            "\n",
            " Current runtime to process row 0 is 0.0036449761000009553 in minutes\n",
            "Processing row 50\n",
            "\n",
            " Current runtime to process row 50 is 0.03625695796666643 in minutes\n",
            "Processing row 100\n",
            "\n",
            " Current runtime to process row 100 is 0.08301622416666797 in minutes\n",
            "Processing row 150\n",
            "\n",
            " Current runtime to process row 150 is 0.11577711826666738 in minutes\n",
            "0.14843455413333306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7obmxmsDNQSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "85947eeb-c5b6-486f-99ae-d99a38d48b9a"
      },
      "cell_type": "code",
      "source": [
        "#0.14843455413333306\n",
        "#0.14843455413333306/(200/len(sentences))\n",
        "\n",
        "200/500"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "850KN9MlMdS1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that the total runtime here is not even 1 minute and we pushed through ~1/25 or less than 5% of our total data. \n",
        ">**If our function's runtime was linear we could predict relatively easily the runtime **\n",
        "\n",
        "Below we will compare the same split runtime when calling the function over a data set that is only 2.5 times the size."
      ]
    },
    {
      "metadata": {
        "id": "KzeVNdVcMJgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "eb6887ae-bd42-48d2-b0f9-c8afc88104cd"
      },
      "cell_type": "code",
      "source": [
        "### Now lets Let's run 500 of our 5000+ rows of data or ~ 1/10th of our total data\n",
        "x = 500\n",
        "test = sentences.iloc[:x,:]\n",
        "#test = sentences\n",
        "#start = timeit.default_timer()\n",
        "\n",
        "# Create our data frame with features. This can take a while to run.\n",
        "word_counts = bow_features(test, common_words)\n",
        "#word_counts.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing row 0\n",
            "\n",
            " Current runtime to process row 0 is 0.01685904453333175 in minutes\n",
            "Processing row 50\n",
            "\n",
            " Current runtime to process row 50 is 0.29471184491666613 in minutes\n",
            "Processing row 100\n",
            "\n",
            " Current runtime to process row 100 is 0.7188084216499988 in minutes\n",
            "Processing row 150\n",
            "\n",
            " Current runtime to process row 150 is 1.030362256766663 in minutes\n",
            "Processing row 200\n",
            "\n",
            " Current runtime to process row 200 is 1.3016407786999973 in minutes\n",
            "Processing row 250\n",
            "\n",
            " Current runtime to process row 250 is 1.5950913646999993 in minutes\n",
            "Processing row 300\n",
            "\n",
            " Current runtime to process row 300 is 1.9154713202833327 in minutes\n",
            "Processing row 350\n",
            "\n",
            " Current runtime to process row 350 is 2.1680122456833297 in minutes\n",
            "Processing row 400\n",
            "\n",
            " Current runtime to process row 400 is 2.4808673214166634 in minutes\n",
            "Processing row 450\n",
            "\n",
            " Current runtime to process row 450 is 2.638271389266667 in minutes\n",
            "2.9994781300666657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nHhPnT_OO5tj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Let's compare the Splits** (Keep in mind that this was the runtime on MY VM it may varry slightly for your VM when the code is executed else where)\n",
        "\n",
        "The first set of data was 200 rows long and took 0.14843455413333306 (minutes) to complete.\n",
        "\n",
        "The second set of data was 500 rows long (2.5 times as large) and took 1.3016407786999973 (minutes) to process the same amount of data. The computational runtime increased by a factor of **8.77**.\n",
        "\n",
        "\n",
        "\n",
        "It's clear to see that this is going to take an EXTREMELY long time to run."
      ]
    },
    {
      "metadata": {
        "id": "bywONF6aPyQk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zRgDQxbFXqbk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Plug in any value for X up to the len(sentences) and compare the splits\n",
        "x = 1000\n",
        "test = sentences.iloc[:x,:]\n",
        "#test = sentences\n",
        "#start = timeit.default_timer()\n",
        "\n",
        "# Create our data frame with features. This can take a while to run.\n",
        "word_counts = bow_features(test, common_words)\n",
        "#word_counts.head()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}