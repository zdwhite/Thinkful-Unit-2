{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Challenge: Iterate and evaluate your classifier",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zdwhite/Thinkful-Unit-2/blob/master/Challenge_Iterate_and_evaluate_your_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "DxiqJE2c2ABW",
        "colab_type": "code",
        "outputId": "3b307559-dc64-416c-d4c3-585a6828c755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "#installs the latest seaborn update to colab runtime\n",
        "!pip install -U seaborn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seaborn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl (208kB)\n",
            "\r\u001b[K    4% |█▋                              | 10kB 16.7MB/s eta 0:00:01\r\u001b[K    9% |███▏                            | 20kB 3.2MB/s eta 0:00:01\r\u001b[K    14% |████▊                           | 30kB 4.5MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 40kB 2.9MB/s eta 0:00:01\r\u001b[K    24% |███████▉                        | 51kB 3.6MB/s eta 0:00:01\r\u001b[K    29% |█████████▌                      | 61kB 4.2MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 71kB 4.8MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 81kB 5.4MB/s eta 0:00:01\r\u001b[K    44% |██████████████▏                 | 92kB 6.0MB/s eta 0:00:01\r\u001b[K    49% |███████████████▊                | 102kB 4.8MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▎              | 112kB 4.8MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 122kB 6.6MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▌           | 133kB 6.6MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 143kB 11.9MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 153kB 12.1MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 163kB 12.1MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▊     | 174kB 12.2MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▍   | 184kB 12.3MB/s eta 0:00:01\r\u001b[K    93% |██████████████████████████████  | 194kB 12.3MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 204kB 37.5MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 215kB 25.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (0.22.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (2.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn) (2018.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (2.3.0)\n",
            "Installing collected packages: seaborn\n",
            "  Found existing installation: seaborn 0.7.1\n",
            "    Uninstalling seaborn-0.7.1:\n",
            "      Successfully uninstalled seaborn-0.7.1\n",
            "Successfully installed seaborn-0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cepaISa72ENr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This restarts the Python runtime so any modules that get installed/upgraded automatically get imported\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3IkMo8XqVrD-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re as re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C6-CN9XJWJNk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Grab and process the raw data.\n",
        "data_path = (\"https://raw.githubusercontent.com/zdwhite/Thinkful-Unit-2/master/Sentiment%20Data%20Sets/yelp_labelled.txt\"\n",
        "            )\n",
        "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
        "sms_raw.columns = ['message', 'sentiment']\n",
        "sms_raw['message']=sms_raw['message'].str.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ZM-pM17pVtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Words of caution\n",
        "\n",
        "A lot of this code is a repition of it self. I did not spend much time creating a function to shorten the length ."
      ]
    },
    {
      "metadata": {
        "id": "B7J3hOmkTzGR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Create 5 different Classifiers (prediction models)\n",
        " \n",
        "\n",
        "1.   Give an output of performance for each model\n",
        "2.   Do any of your classifiers seem to overfit?\n",
        "3.   Which seem to perform the best? Why?\n",
        "4.   Which features seemed to be most impactful to performance?\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RyWGsLnmWNin",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#before we tackle the NB model Let's get a sense of some key words\n",
        "\n",
        "#Split\n",
        "good_sentiment=sms_raw[sms_raw['sentiment']==1]['message'].apply(lambda x:x.split())\n",
        "\n",
        "#Looking through our bag of words, there are lots of punctuations and possibly special characters let's get rid of those\n",
        "good_sentiment = pd.DataFrame(good_sentiment).apply(lambda x: pd.Series(x['message']),axis=1).stack().reset_index(level=1, drop=True).reset_index()\n",
        "\n",
        "good_sentiment.columns=['Index','Words']\n",
        "\n",
        "good_sentiment['Words']=good_sentiment['Words'].apply(lambda x: re.sub('[^A-Za-z0-9]+','',x))\n",
        "\n",
        "GSWC = pd.DataFrame(good_sentiment['Words'].value_counts().reset_index())\n",
        "GSWC['sentiment']='Good'\n",
        "\n",
        "#Split\n",
        "bad_sentiment=sms_raw[sms_raw['sentiment']==0]['message'].apply(lambda x:x.split())\n",
        "\n",
        "bad_sentiment = pd.DataFrame(bad_sentiment).apply(lambda x: pd.Series(x['message']),axis=1).stack().reset_index(level=1, drop=True).reset_index()\n",
        "\n",
        "bad_sentiment.columns=['Index','Words']\n",
        "\n",
        "#Combine\n",
        "#Looking through our bag of words, there are lots of punctuations and possibly special characters let's get rid of those\n",
        "bad_sentiment['Words']=bad_sentiment['Words'].apply(lambda x: re.sub('[^A-Za-z0-9]+','',x))\n",
        "\n",
        "bad_sentiment['Words'].value_counts()\n",
        "\n",
        "BSWC = pd.DataFrame(bad_sentiment['Words'].value_counts().reset_index())\n",
        "BSWC['sentiment']='Bad'\n",
        "\n",
        "Bag_of_words=GSWC.append(BSWC)\n",
        "Bag_of_words.columns=['Word','Count','Sentiment']\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VBTIFWd7aL2r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#implement outer join\n",
        "\n",
        "#pd.merge(adf, bdf,how='outer', on='x1')\n",
        "\n",
        "Good= Bag_of_words[Bag_of_words[\"Sentiment\"] == \"Good\"]\n",
        "Bad = Bag_of_words[Bag_of_words[\"Sentiment\"] == \"Bad\"]\n",
        "\n",
        "test = pd.merge(Good,Bad,how='outer',on='Word',suffixes =[\"_Good\",\"_Bad\"]).set_index('Word')\n",
        "\n",
        "#test.fillna(0,inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zvr-nD1IWL_F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############# CLASSIFIER 1 ( The dumb version)\n",
        "#In this version instead of filtering keywords we are going to grab words that are in bad or good\n",
        "### Output ###\n",
        "# Confusion Matrix for both good and bad predictions\n",
        "# Accuracy for both\n",
        "\n",
        "###https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n",
        "##### Recursive Feature selection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zh5ZxS0EGE1Z",
        "colab_type": "code",
        "outputId": "9b78e4fe-2334-471c-9c26-676c1d891c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# all keywords that appear to have bad sentiment\n",
        "## Bad Key words\n",
        "\n",
        "keywords = test[test.iloc[:,2]>0].iloc[:,2].index.values\n",
        "\n",
        "#create features in sms_raw\n",
        "for key in keywords:\n",
        "    sms_raw[str(key)] = sms_raw.message.str.contains(\n",
        "        ' ' + str(key) + ' ',\n",
        "        case=False\n",
        ")\n",
        "\n",
        "#create data for NB classifier\n",
        "#Classifies Sentiment column into True for Bad sentiment and False for Good sentiment\n",
        "sms_raw['sentiment'] = (sms_raw['sentiment'] == 0)\n",
        "data = sms_raw[keywords]\n",
        "target = sms_raw['sentiment']\n",
        "\n",
        "\n",
        "\n",
        "#run the classifier through the Bernoulli Naive Bayes model    \n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "y_pred = bnb.fit(data, target).predict(data)\n",
        "\n",
        "# Display our results.\n",
        "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
        "    data.shape[0],\n",
        "    (target != y_pred).sum()\n",
        "))\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of mislabeled points out of a total 1000 points : 221\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[478,  22],\n",
              "       [199, 301]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "hn4Slb2Zmj69",
        "colab_type": "code",
        "outputId": "4efba412-0654-455e-d23b-8f7a6f5bb6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Test your model with different holdout groups.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
        "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With 20% Holdout: 0.59\n",
            "Testing on Sample: 0.779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vJzpV0v3mzwv",
        "colab_type": "code",
        "outputId": "f8aa5324-69d4-43c4-efb8-c4aa91fd2267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(bnb, data, target, cv=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.68, 0.62, 0.7 , 0.64, 0.61, 0.59, 0.57, 0.66, 0.59, 0.64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "metadata": {
        "id": "PmU1MjN2m3m4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we can see from the hold out tests and the cross validation accuracy scores this model is not very accurate.\n",
        "\n",
        "Lets check the converse for Good seniment words and see how it performs"
      ]
    },
    {
      "metadata": {
        "id": "F4Dm--dQkCaX",
        "colab_type": "code",
        "outputId": "7d040583-0698-47d2-c6e9-ea771e505823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "################# RE-RUN same model using good sentiment keywords and show accuracy\n",
        "# Grab and process the raw data.\n",
        "data_path = (\"https://raw.githubusercontent.com/zdwhite/Thinkful-Unit-2/master/Sentiment%20Data%20Sets/yelp_labelled.txt\"\n",
        "            )\n",
        "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
        "sms_raw.columns = ['message', 'sentiment']\n",
        "sms_raw['message']=sms_raw['message'].str.lower()\n",
        "\n",
        "\n",
        "\n",
        "#### Re-create the same classifier but using Good keywords\n",
        "\n",
        "## Good Keywords\n",
        "keywords = test[test.iloc[:,0]>0].iloc[:,0].index.values\n",
        "\n",
        "#create features in sms_raw\n",
        "for key in keywords:\n",
        "    sms_raw[str(key)] = sms_raw.message.str.contains(\n",
        "        ' ' + str(key) + ' ',\n",
        "        case=False\n",
        ")\n",
        "\n",
        "#create data for NB classifier\n",
        "#Classifies Sentiment column into True for Bad sentiment and False for Good sentiment\n",
        "sms_raw['sentiment'] = (sms_raw['sentiment'] == 1)\n",
        "data = sms_raw[keywords]\n",
        "target = sms_raw['sentiment']\n",
        "\n",
        "\n",
        "\n",
        "#run the classifier through the Bernoulli Naive Bayes model    \n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "y_pred = bnb.fit(data, target).predict(data)\n",
        "\n",
        "# Display our results.\n",
        "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
        "    data.shape[0],\n",
        "    (target != y_pred).sum()\n",
        "))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of mislabeled points out of a total 1000 points : 163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[476,  24],\n",
              "       [139, 361]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "a301ff3d-5d38-41ac-b9df-faabee57a38e",
        "id": "r_lTrXj5nDzw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Test your model with different holdout groups.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
        "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With 20% Holdout: 0.63\n",
            "Testing on Sample: 0.837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "0c8acc7e-255e-4293-eb71-7225af2b460e",
        "id": "CycJh0RtnDz1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(bnb, data, target, cv=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.68, 0.66, 0.69, 0.66, 0.65, 0.65, 0.65, 0.71, 0.61, 0.7 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "metadata": {
        "id": "pMZwx8VvmFH8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We notice that the cross validation of the dummy classifier for Bad Keywords performs fairly similarly to that of using good seniment keywords\n",
        "\n",
        "BAD Seniment Key word Accuracy array = [0.68, 0.62, 0.7 , 0.64, 0.61, 0.59, 0.57, 0.66, 0.59, 0.64]\n",
        "\n",
        "GOOD Seniment Key word Accuracy array = [0.68, 0.66, 0.69, 0.66, 0.65, 0.65, 0.65, 0.71, 0.61, 0.7 ]\n",
        "\n",
        "Let's try a smarter way of chosing keywords"
      ]
    },
    {
      "metadata": {
        "id": "SbgEd4ZYOJ0C",
        "colab_type": "code",
        "outputId": "39d0e808-d419-484b-a567-daef4767076d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "#### This classifier filters out keywords by making sure their count is larger than the other sentiment count\n",
        "# This will filter for Bad Keywords\n",
        "\n",
        "#Bag_of_words.describe()\n",
        "#std = Bag_of_words.describe()['Count'][2]\n",
        "\n",
        "keywords = []\n",
        "for word in test.index.values:\n",
        "  #print(test[test['Word']==word])\n",
        "  #if test.loc[word][0] < test.loc[word][2]+std*2:\n",
        "  if test.loc[word][0] < test.loc[word][2]:\n",
        "    keywords.append(word)\n",
        "  #if (test.loc[word][0]) == 0 & (test.loc[word][2] > 2):\n",
        "    #if word not in keyword:\n",
        "      #keyword.append(word)\n",
        "\n",
        "# Grab and process the raw data.\n",
        "data_path = (\"https://raw.githubusercontent.com/zdwhite/Thinkful-Unit-2/master/Sentiment%20Data%20Sets/yelp_labelled.txt\"\n",
        "            )\n",
        "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
        "sms_raw.columns = ['message', 'sentiment']\n",
        "sms_raw['message']=sms_raw['message'].str.lower()\n",
        "\n",
        "#create features in sms_raw\n",
        "for key in keywords:\n",
        "    sms_raw[str(key)] = sms_raw.message.str.contains(\n",
        "        ' ' + str(key) + ' ',\n",
        "        case=False\n",
        ")\n",
        "\n",
        "#create data for NB classifier\n",
        "#Classifies Sentiment column into True for Bad sentiment and False for Good sentiment\n",
        "sms_raw['sentiment'] = (sms_raw['sentiment'] == 0)\n",
        "data = sms_raw[keywords]\n",
        "target = sms_raw['sentiment']\n",
        "\n",
        "\n",
        "\n",
        "#run the classifier through the Bernoulli Naive Bayes model    \n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "y_pred = bnb.fit(data, target).predict(data)\n",
        "\n",
        "# Display our results.\n",
        "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
        "    data.shape[0],\n",
        "    (target != y_pred).sum()\n",
        "))\n",
        "\n",
        "# Test your model with different holdout groups.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
        "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(target, y_pred))\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(bnb, data, target, cv=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of mislabeled points out of a total 1000 points : 300\n",
            "With 20% Holdout: 0.575\n",
            "Testing on Sample: 0.7\n",
            "[[413  87]\n",
            " [213 287]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.71, 0.63, 0.7 , 0.57, 0.59, 0.66, 0.6 , 0.67, 0.58, 0.66])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "xsZzKgA6pPBz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model got worse ... that's progress we all can get behind."
      ]
    },
    {
      "metadata": {
        "id": "bebLE2zJwDwY",
        "colab_type": "code",
        "outputId": "5c466333-5dc4-4e14-a9b7-6409212325f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "#### This classifier filters out keywords by making sure their count is larger than the other sentiment count\n",
        "# This will filter for Good Keywords\n",
        "\n",
        "#Bag_of_words.describe()\n",
        "#std = Bag_of_words.describe()['Count'][2]\n",
        "\n",
        "keywords = []\n",
        "for word in test.index.values:\n",
        "  #print(test[test['Word']==word])\n",
        "  #if test.loc[word][0] < test.loc[word][2]+std*2:\n",
        "  if test.loc[word][0] > test.loc[word][2]:\n",
        "    keywords.append(word)\n",
        "  #if (test.loc[word][0]) == 0 & (test.loc[word][2] > 2):\n",
        "    #if word not in keyword:\n",
        "      #keyword.append(word)\n",
        "\n",
        "# Grab and process the raw data.\n",
        "data_path = (\"https://raw.githubusercontent.com/zdwhite/Thinkful-Unit-2/master/Sentiment%20Data%20Sets/yelp_labelled.txt\"\n",
        "            )\n",
        "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
        "sms_raw.columns = ['message', 'sentiment']\n",
        "sms_raw['message']=sms_raw['message'].str.lower()\n",
        "\n",
        "#create features in sms_raw\n",
        "for key in keywords:\n",
        "    sms_raw[str(key)] = sms_raw.message.str.contains(\n",
        "        ' ' + str(key) + ' ',\n",
        "        case=False\n",
        ")\n",
        "\n",
        "#create data for NB classifier\n",
        "#Classifies Sentiment column into True for Bad sentiment and False for Good sentiment\n",
        "sms_raw['sentiment'] = (sms_raw['sentiment'] == 1)\n",
        "data = sms_raw[keywords]\n",
        "target = sms_raw['sentiment']\n",
        "\n",
        "\n",
        "\n",
        "#run the classifier through the Bernoulli Naive Bayes model    \n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "y_pred = bnb.fit(data, target).predict(data)\n",
        "\n",
        "# Display our results.\n",
        "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
        "    data.shape[0],\n",
        "    (target != y_pred).sum()\n",
        "))\n",
        "\n",
        "# Test your model with different holdout groups.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
        "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(bnb, data, target, cv=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of mislabeled points out of a total 1000 points : 253\n",
            "With 20% Holdout: 0.61\n",
            "Testing on Sample: 0.747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.55, 0.6 , 0.64, 0.6 , 0.61, 0.59, 0.68, 0.59, 0.62, 0.66])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "metadata": {
        "id": "FfQ_mj13wKdT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model keeps getting worse! Let's take the top 10 most frequently used words in good or bad as long as their frequency is higher than the other"
      ]
    },
    {
      "metadata": {
        "id": "9kts_gGNwn51",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fSw56CKTwPtW",
        "colab_type": "code",
        "outputId": "5d92eae0-57ed-47a5-bb0c-e3f8a2d2e252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "#### This classifier filters out keywords by making sure their count is larger than the other sentiment count\n",
        "# This will filter for Bad Keywords\n",
        "\n",
        "#Bag_of_words.describe()\n",
        "#std = Bag_of_words.describe()['Count'][2]\n",
        "\n",
        "keywords = []\n",
        "i=0\n",
        "for word in test.index.values:\n",
        "  #print(test[test['Word']==word])\n",
        "  #if test.loc[word][0] < test.loc[word][2]+std*2:\n",
        "  if test.loc[word][2]>test.loc[word][0] :\n",
        "    keywords.append(word)\n",
        "    i +=1\n",
        "  if i==len(test.index.values):\n",
        "    break\n",
        "\n",
        "\n",
        "# Grab and process the raw data.\n",
        "data_path = (\"https://raw.githubusercontent.com/zdwhite/Thinkful-Unit-2/master/Sentiment%20Data%20Sets/yelp_labelled.txt\"\n",
        "            )\n",
        "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
        "sms_raw.columns = ['message', 'sentiment']\n",
        "sms_raw['message']=sms_raw['message'].str.lower()\n",
        "\n",
        "#create features in sms_raw\n",
        "for key in keywords:\n",
        "    sms_raw[str(key)] = sms_raw.message.str.contains(\n",
        "        ' ' + str(key) + ' ',\n",
        "        case=False\n",
        ")\n",
        "\n",
        "#create data for NB classifier\n",
        "#Classifies Sentiment column into True for Bad sentiment and False for Good sentiment\n",
        "sms_raw['sentiment'] = (sms_raw['sentiment'] == 0)\n",
        "data = sms_raw[keywords]\n",
        "target = sms_raw['sentiment']\n",
        "\n",
        "\n",
        "\n",
        "#run the classifier through the Bernoulli Naive Bayes model    \n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "y_pred = bnb.fit(data, target).predict(data)\n",
        "\n",
        "# Display our results.\n",
        "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
        "    data.shape[0],\n",
        "    (target != y_pred).sum()\n",
        "))\n",
        "\n",
        "# Test your model with different holdout groups.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
        "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(target, y_pred))\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(bnb, data, target, cv=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of mislabeled points out of a total 1000 points : 300\n",
            "With 20% Holdout: 0.575\n",
            "Testing on Sample: 0.7\n",
            "[[413  87]\n",
            " [213 287]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.71, 0.63, 0.7 , 0.57, 0.59, 0.66, 0.6 , 0.67, 0.58, 0.66])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "o-sUp4LGyWk2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This makes sense we have limited keywords for features, the difference between 10 features and 100 features isn't much different.\n",
        "\n",
        "Before we pull in all the data sets, lets try a smart approach at filtering keyword features and add in a feature to look for words that are capitalized.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mjiUq-05yV07",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### New classifier using two data sets for generating keywords tested against our yelp data-set\n",
        "\n",
        "\n",
        "########## Yelp Reviews\n",
        "data_path1 = (\"https://raw.githubusercontent.com/zdwhite/Thinkful-Unit-2/master/Sentiment%20Data%20Sets/yelp_labelled.txt\")\n",
        "sms_raw1 = pd.read_csv(data_path1, delimiter= '\\t', header=None)\n",
        "sms_raw1.columns = ['message', 'sentiment']\n",
        "\n",
        "########## Amazon reviews\n",
        "\n",
        "### Something in this data set isn't allowing me to \n",
        "data_path2 = (\"https://raw.githubusercontent.com/zdwhite/Thinkful-Unit-2/master/Sentiment%20Data%20Sets/amazon_cells_labelled.txt\")\n",
        "\n",
        "sms_raw2 = pd.read_csv(data_path2, delimiter= '\\t', header=None)\n",
        "sms_raw2.columns = ['message', 'sentiment']\n",
        "\n",
        "sms_raw2['message']=sms_raw2['message'].str.lower()\n",
        "\n",
        "########## IMDB reviews\n",
        "data_path3 = (\"https://raw.githubusercontent.com/zdwhite/Thinkful-Unit-2/master/Sentiment%20Data%20Sets/imdb_labelled.txt\")\n",
        "\n",
        "sms_raw3 = pd.read_csv(data_path3, delimiter= '\\t', header=None)\n",
        "sms_raw3.columns = ['message', 'sentiment']\n",
        "\n",
        "sms_raw3['message']=sms_raw3['message'].str.lower()\n",
        "\n",
        "\n",
        "sms_raw = pd.concat([sms_raw1,sms_raw3])\n",
        "#sms_raw = sms_raw1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwFuCvJdGFP4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jpsIx9-VhLcP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#before we tackle the NB model Let's get a sense of some key words\n",
        "\n",
        "#Split\n",
        "good_sentiment=sms_raw[sms_raw['sentiment']==1]['message'].apply(lambda x:x.split())\n",
        "\n",
        "#Looking through our bag of words, there are lots of punctuations and possibly special characters let's get rid of those\n",
        "good_sentiment = pd.DataFrame(good_sentiment).apply(lambda x: pd.Series(x['message']),axis=1).stack().reset_index(level=1, drop=True).reset_index()\n",
        "\n",
        "good_sentiment.columns=['Index','Words']\n",
        "\n",
        "good_sentiment['Words']=good_sentiment['Words'].apply(lambda x: re.sub('[^A-Za-z0-9]+','',x))\n",
        "\n",
        "GSWC = pd.DataFrame(good_sentiment['Words'].value_counts().reset_index())\n",
        "GSWC['sentiment']='Good'\n",
        "\n",
        "#Split\n",
        "bad_sentiment=sms_raw[sms_raw['sentiment']==0]['message'].apply(lambda x:x.split())\n",
        "\n",
        "bad_sentiment = pd.DataFrame(bad_sentiment).apply(lambda x: pd.Series(x['message']),axis=1).stack().reset_index(level=1, drop=True).reset_index()\n",
        "\n",
        "bad_sentiment.columns=['Index','Words']\n",
        "\n",
        "#Combine\n",
        "#Looking through our bag of words, there are lots of punctuations and possibly special characters let's get rid of those\n",
        "bad_sentiment['Words']=bad_sentiment['Words'].apply(lambda x: re.sub('[^A-Za-z0-9]+','',x))\n",
        "\n",
        "bad_sentiment['Words'].value_counts()\n",
        "\n",
        "BSWC = pd.DataFrame(bad_sentiment['Words'].value_counts().reset_index())\n",
        "BSWC['sentiment']='Bad'\n",
        "\n",
        "Bag_of_words=GSWC.append(BSWC)\n",
        "Bag_of_words.columns=['Word','Count','Sentiment']\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FD5QQPEYhLcR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#implement outer join\n",
        "\n",
        "#pd.merge(adf, bdf,how='outer', on='x1')\n",
        "\n",
        "Good= Bag_of_words[Bag_of_words[\"Sentiment\"] == \"Good\"]\n",
        "Bad = Bag_of_words[Bag_of_words[\"Sentiment\"] == \"Bad\"]\n",
        "\n",
        "test = pd.merge(Good,Bad,how='outer',on='Word',suffixes =[\"_Good\",\"_Bad\"]).set_index('Word')\n",
        "\n",
        "test.fillna(0,inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3LDJfbMBSPX7",
        "colab_type": "code",
        "outputId": "b5accf14-de0c-4760-96e1-650eb8836657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "#### Let's use our filtering classifier and generate our bad sentiment keywords\n",
        "# This will filter for Bad Keywords\n",
        "\n",
        "#Bag_of_words.describe()\n",
        "std = Bag_of_words.describe()['Count'][2]\n",
        "\n",
        "keywords = []\n",
        "for word in test.index.values:\n",
        "  #print(test[test['Word']==word])\n",
        "  #if test.loc[word][0] < test.loc[word][2]+std*2:\n",
        "  if test.loc[word][2] >test.loc[word][0] :\n",
        "    keywords.append(word)\n",
        "\n",
        "\n",
        "# Grab and process the raw data.\n",
        "sms_raw=sms_raw1\n",
        "\n",
        "#create features in sms_raw\n",
        "for key in keywords:\n",
        "    sms_raw[str(key)] = sms_raw.message.str.contains(\n",
        "        ' ' + str(key) + ' ',\n",
        "        case=False\n",
        ")\n",
        "\n",
        "#create data for NB classifier\n",
        "#Classifies Sentiment column into True for Bad sentiment and False for Good sentiment\n",
        "sms_raw['sentiment'] = (sms_raw['sentiment'] == 0)\n",
        "data = sms_raw[keywords]\n",
        "target = sms_raw['sentiment']\n",
        "\n",
        "\n",
        "\n",
        "#run the classifier through the Bernoulli Naive Bayes model    \n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "y_pred = bnb.fit(data, target).predict(data)\n",
        "\n",
        "# Display our results.\n",
        "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
        "    data.shape[0],\n",
        "    (target != y_pred).sum()\n",
        "))\n",
        "\n",
        "# Test your model with different holdout groups.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
        "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(target, y_pred))\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(bnb, data, target, cv=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of mislabeled points out of a total 1000 points : 229\n",
            "With 20% Holdout: 0.665\n",
            "Testing on Sample: 0.771\n",
            "[[470  30]\n",
            " [199 301]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.74, 0.67, 0.76, 0.63, 0.6 , 0.64, 0.66, 0.73, 0.64, 0.71])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "leZxZqvdsZUI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Do any of your classifiers seem to overfit? \n",
        ">None of them\n",
        "\n",
        "##Which seem to perform the best? Why?\n",
        ">It's splitting hairs on which model performs the best when comparing their cross validation arrays\n",
        "\n",
        "##Which features seemed to be most impactful to performance?\n",
        ">I'm unsure which features seem to be the most impactful in terms of keyword selection.\n",
        "\n",
        ">I don't know why there is no improvement other than the fact that individual key words aren't all that great as features for NB models.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4NE2TvpNtduq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#####################################IGNORE CODE BELOW THIS LINE \n",
        "#----------------------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_2O55wZRcORA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###### I can't remember why I coded this TBH for now it's depretiated code until proven otherwise\n",
        "\n",
        "#df = sms_raw\n",
        "#code 1 & 0 to Good and Bad\n",
        "\n",
        "df = pd.DataFrame([])\n",
        "#df.loc[df['sentiment']==1,'sentiment'],df.loc[df['sentiment']==0,'sentiment'] = 'Good','Bad'\n",
        "df['message']=sms_raw['message'].apply(lambda x:x.split())\n",
        "df['sentiment']=sms_raw['sentiment']\n",
        "\n",
        "#Bag_of_words = pd.DataFrame([],columns=['Word','Count','Sentiment'])\n",
        "\n",
        "\n",
        "\n",
        "#df = pd.DataFrame([])\n",
        "#df.loc[df['sentiment']==1,'sentiment'],df.loc[df['sentiment']==0,'sentiment'] = 'Good','Bad'\n",
        "df['message']=sms_raw['message'].apply(lambda x:x.split()) \n",
        "\n",
        "df=df.set_index('sentiment',append=True).apply(lambda x: pd.Series(x['message']),axis=1).stack()\n",
        "df=pd.DataFrame(df.reset_index().reset_index())\n",
        "\n",
        "df=df.drop(['index','level_0','level_2'],axis=1)\n",
        "df.columns=['sentiment','messages']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SYNRmJWcVtyi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###### Attempts to make a big ass function that I could just call for modeling not worth the effort ATM\n",
        "#assumes data is precoded to 0 or 1\n",
        "# \n",
        "#def model ()\n",
        "\n",
        "#inputs ( data, messages ,key)\n",
        "# split data\n",
        "\n",
        "#def split_data(data,messages,key):\n",
        "#  # good == 1  ,bad == 0\n",
        "#  Bag_of_words = pd.DataFrame([],columns=['Word','Count','Sentiment'])\n",
        "#  df=data\n",
        "#  \n",
        "#  df.loc[df['sentiment']==1,'sentiment'],df.loc[df['sentiment']==0,'sentiment'] = 'Good','Bad'\n",
        "#  df[messages]=df[messages].apply(lambda x:x.split())\n",
        "#  test=df.apply(lambda x: pd.DataFrame(x),axis=1).stack().reset_index(level=1, drop=True).reset_index()\n",
        "#  return test"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}